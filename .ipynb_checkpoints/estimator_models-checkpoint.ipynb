{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with tf.data and tf.estimators\n",
    "\n",
    "Since the recent 2018 Google I/O I meant to do a tutorial on the new data pipelines of tensorflow and the estimator class that they introduced a couple of verions ago. Google is pushing tensorflow to be an easy to use framework without a steep learning curve - at least in order to achieve elementary results. Both libraries that we are going to showcase in this notebook are striving for that. Tf.data replaces the old fashioned way of feed_dict within the tf.session and streamlines the data input flow. Tf.estimator acts as a blanket for all deep learning models that have tensorflow under the hood. It takes care of training, evaluation and prediction with wrapper functions on top of your model. Also we are going to see one of the out of the box classifiers that Google has developed DNNClassifier and test how good it performs.\n",
    "\n",
    "In order to examine these libraries we are going to tackle the Kaggle problem of the Titanic. We will try to predict based on features such as how much the Titanic ticket cost, the age, the ticket class etc if the passenger survived or not. Most of the good solutions on Kaggle achieve around 75-85% accuracy in this problem with extensive feature engineering. Here we are not going to bother with feature engineering since our purpose is not to break the Kaggle record. Let's take a look of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                             name     sex  \\\n",
       "0         1       1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1         1       1                   Allison, Master. Hudson Trevor    male   \n",
       "2         0       1                     Allison, Miss. Helen Loraine  female   \n",
       "3         0       1             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4         0       1  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare  \n",
       "0  29.0000      0      0   24160  211.3375  \n",
       "1   0.9167      1      2  113781  151.5500  \n",
       "2   2.0000      1      2  113781  151.5500  \n",
       "3  30.0000      1      2  113781  151.5500  \n",
       "4  25.0000      1      2  113781  151.5500  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset on memory and show first 5 records\n",
    "data = pd.read_csv(\"/Users/Blackbak/giannis_home/python_folder/titanic_dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test for us to evaluate the generalization of our models\n",
    "train = data.iloc[:int(data.shape[0]*0.8)]\n",
    "test = data.iloc[int(data.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that we need to do is to specify our input pipeline. The pipeline ties with the estimator class because we need to feed the estimator data in a specific way. The estimator class takes as an argument an input function that returns the next data to be trained or evaluated. So basically what we need is a generator function that outputs the next next batch of data for batch training and testing or the next data point for online. If you are not familiar with generators I would suggest seeing [this youtube video](https://www.youtube.com/watch?v=cKPlPJyQrt4) (actually I would suggest it to everyone regardless). Thankfully Google has provided us with the nesessary tools that make this task very easy. But first things first we need to specify the data that we need to load. Depending on how the data are stored there are different functions to load the data into the tf.data.Dataset class. We see below the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data are stored in the default format of tensorflow TFRecords\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = tf.data.TFRecordDataset(files)\n",
    "# If the data are store in one or multiple csv files\n",
    "dataset = tf.contrib.data.make_csv_dataset(\"*.csv\", # path to the csv file/files\n",
    "                                       batch_size=32, # have to specify batch size in this step\n",
    "                                       column_names=[\"features\", \"that\", \"are\", \"useful\"],\n",
    "                                       label_name=\"label_column\")\n",
    "# If the data are in memory already in a dictionary\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data are loaded into a tf.data.Dataset form our goal is to develop the generator function. Before we proceed with making our data iterable we need to specify some key parameters on how we consume them such as the number of epochs, the batch size, if we shuffle after each epoch or if we want to manipulate the input. As in their [presentation at Google](https://www.youtube.com/watch?v=uIcqeP7MFH0&t=270s) the usual data pipeline would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000) # 1000 is the shuffle buffer size where it samples from\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "# if some pre-processing is needed we can do map and filter functions with the help of lambda\n",
    "# the downside is that it is somewhat complex\n",
    "dataset = dataset.map(lambda x: tf.parse_single_example(x, features)) \n",
    "dataset = dataset.batch(batch_size)\n",
    "# here we make the data iterable and call the next batch\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point what we only need to do is to wrap these operations in a function to be able to get parsed as an argument in the estimator decleration. Also we will need two functions, one for training and one for evaluating. We could have one and parse the data as an argument but this way is a bit more clear. We are going to use only numerical features to keep the notebook short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train[[\"pclass\", \"fare\", \"age\", \"sibsp\", \"parch\"]].to_dict(\"list\"), \n",
    "                                                  train[\"survived\"].values))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(100)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    feat_next, label_next = iterator.get_next()\n",
    "    return feat_next, label_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((test[[\"pclass\", \"fare\", \"age\", \"sibsp\", \"parch\"]].to_dict(\"list\"), \n",
    "                                                  test[\"survived\"].values))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(100)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    feat_next, label_next = iterator.get_next()\n",
    "    return feat_next, label_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define our features for the estimator to understand its input. Basically with this step we make sure that values get connected to the corresponding input and that different types of input gets handled accordingly e.g. categorical entries get translated to one-hot encodings. More on the features at the [tensorflow docs](https://www.tensorflow.org/get_started/feature_columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name = [\"pclass\", \"fare\", \"age\", \"sibsp\", \"parch\"]\n",
    "my_feature_columns = []\n",
    "for name in feat_name:\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the estimator class. Google has developed a handful of predefined estimators to make our life a bit easier. We are going to showcase the DNNClassifier model, which is what the name suggests: a feed forward classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmppsx1900f\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmppsx1900f', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10a230630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n",
    "                                        hidden_units=[1000,1000],\n",
    "                                      dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmppsx1900f/model.ckpt.\n",
      "INFO:tensorflow:loss = 194.63202, step = 1\n",
      "INFO:tensorflow:global_step/sec: 50.324\n",
      "INFO:tensorflow:loss = 67.78077, step = 101 (1.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.2207\n",
      "INFO:tensorflow:loss = 62.972076, step = 201 (1.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.7792\n",
      "INFO:tensorflow:loss = 63.67186, step = 301 (1.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.0776\n",
      "INFO:tensorflow:loss = 64.02928, step = 401 (1.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.2241\n",
      "INFO:tensorflow:loss = 63.789528, step = 501 (1.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 55.9744\n",
      "INFO:tensorflow:loss = 64.28005, step = 601 (1.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.4468\n",
      "INFO:tensorflow:loss = 62.797832, step = 701 (1.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.4926\n",
      "INFO:tensorflow:loss = 64.018936, step = 801 (1.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8435\n",
      "INFO:tensorflow:loss = 64.99931, step = 901 (2.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1407\n",
      "INFO:tensorflow:loss = 59.514896, step = 1001 (2.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.6324\n",
      "INFO:tensorflow:loss = 65.64246, step = 1101 (2.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.8222\n",
      "INFO:tensorflow:loss = 57.61671, step = 1201 (2.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.2843\n",
      "INFO:tensorflow:loss = 57.705513, step = 1301 (2.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.9542\n",
      "INFO:tensorflow:loss = 56.1115, step = 1401 (2.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6494\n",
      "INFO:tensorflow:loss = 64.49755, step = 1501 (2.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.2238\n",
      "INFO:tensorflow:loss = 64.67319, step = 1601 (2.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0495\n",
      "INFO:tensorflow:loss = 61.380512, step = 1701 (2.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.441\n",
      "INFO:tensorflow:loss = 64.450134, step = 1801 (1.983 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3973\n",
      "INFO:tensorflow:loss = 56.074, step = 1901 (2.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7706\n",
      "INFO:tensorflow:loss = 63.082817, step = 2001 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.8167\n",
      "INFO:tensorflow:loss = 60.534943, step = 2101 (2.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4023\n",
      "INFO:tensorflow:loss = 58.265648, step = 2201 (2.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.6467\n",
      "INFO:tensorflow:loss = 68.471176, step = 2301 (2.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.628\n",
      "INFO:tensorflow:loss = 53.29716, step = 2401 (1.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3244\n",
      "INFO:tensorflow:loss = 63.285633, step = 2501 (2.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.95\n",
      "INFO:tensorflow:loss = 58.008884, step = 2601 (1.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.0401\n",
      "INFO:tensorflow:loss = 64.027405, step = 2701 (1.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6456\n",
      "INFO:tensorflow:loss = 66.76814, step = 2801 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.8796\n",
      "INFO:tensorflow:loss = 63.259502, step = 2901 (2.004 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmppsx1900f/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 64.38674.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x10a230358>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-10-14:39:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmppsx1900f/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [20/200]\n",
      "INFO:tensorflow:Evaluation [40/200]\n",
      "INFO:tensorflow:Evaluation [60/200]\n",
      "INFO:tensorflow:Evaluation [80/200]\n",
      "INFO:tensorflow:Evaluation [100/200]\n",
      "INFO:tensorflow:Evaluation [120/200]\n",
      "INFO:tensorflow:Evaluation [140/200]\n",
      "INFO:tensorflow:Evaluation [160/200]\n",
      "INFO:tensorflow:Evaluation [180/200]\n",
      "INFO:tensorflow:Evaluation [200/200]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-10-14:39:14\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.78255, accuracy_baseline = 0.79015, auc = 0.5815983, auc_precision_recall = 0.28891575, average_loss = 0.55284804, global_step = 3000, label/mean = 0.20985, loss = 55.284805, precision = 0.4500657, prediction/mean = 0.32641548, recall = 0.16321182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.78255,\n",
       " 'accuracy_baseline': 0.79015,\n",
       " 'auc': 0.5815983,\n",
       " 'auc_precision_recall': 0.28891575,\n",
       " 'average_loss': 0.55284804,\n",
       " 'label/mean': 0.20985,\n",
       " 'loss': 55.284805,\n",
       " 'precision': 0.4500657,\n",
       " 'prediction/mean': 0.32641548,\n",
       " 'recall': 0.16321182,\n",
       " 'global_step': 3000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=eval_input_fn, steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models are nice out of the box solutions for rapid prototyping or for people that are not tha familiar with machine learning. In most cases we will need to specify our own model to solve the specific task at hand e.g. for image classification we will need convolutional layers with pooling. In this example we are going to formulate another feed forward network but we will use batch normalization on the layers. The model is defined as a function that outputs different outcomes based on the mode that it is in. Each estimator has 3 modes: training, evaluating and predicting - tf.estimator.ModeKeys.(TRAIN/EVAL/PREDICT). In the model function my_model_fn we are going to go through the different steps that we must specify in order to comply with the estimator form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the layer that we are going to use as hidden\n",
    "def dnn_layer(inputs, unit_num, activation, d_rate, mode):\n",
    "    bn = tf.layers.batch_normalization(inputs=inputs)\n",
    "    nn = tf.layers.dense(inputs=bn, units=unit_num, activation=activation)\n",
    "    dn = tf.layers.dropout(nn, rate=d_rate, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    return dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_fn(features, labels, mode, params):\n",
    "    # Always the first step of the model function is to connect input and feature definitions\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # Define the computation graph for forward pass\n",
    "    for hid_num in params[\"hidden_units\"]:\n",
    "        net = dnn_layer(inputs=net, unit_num=hid_num, activation=tf.nn.leaky_relu, d_rate=0.5, mode=mode)\n",
    "    logits = tf.layers.dense(inputs=net, units=params[\"n_classes\"])\n",
    "    # Prediction part\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "            \"auc\": tf.metrics.auc(labels=labels, predictions=predictions[\"classes\"])}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmp1tui31ju\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmp1tui31ju', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1235e7908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model_fn,\n",
    "    params={\n",
    "        'feature_columns': my_feature_columns,\n",
    "        'hidden_units': [1000, 1000],\n",
    "        'n_classes': 2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmp1tui31ju/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmp1tui31ju/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6218874, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 41.134\n",
      "INFO:tensorflow:loss = 0.6308135, step = 3101 (2.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4112\n",
      "INFO:tensorflow:loss = 0.59997797, step = 3201 (2.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.8712\n",
      "INFO:tensorflow:loss = 0.55855, step = 3301 (2.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3987\n",
      "INFO:tensorflow:loss = 0.68547094, step = 3401 (2.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.9739\n",
      "INFO:tensorflow:loss = 0.58312017, step = 3501 (2.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.8226\n",
      "INFO:tensorflow:loss = 0.56016314, step = 3601 (2.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.9154\n",
      "INFO:tensorflow:loss = 0.5868153, step = 3701 (3.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8304\n",
      "INFO:tensorflow:loss = 0.6282436, step = 3801 (2.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.4257\n",
      "INFO:tensorflow:loss = 0.6056875, step = 3901 (2.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1671\n",
      "INFO:tensorflow:loss = 0.58671457, step = 4001 (2.034 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7573\n",
      "INFO:tensorflow:loss = 0.6089299, step = 4101 (2.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.8459\n",
      "INFO:tensorflow:loss = 0.6326588, step = 4201 (2.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.9588\n",
      "INFO:tensorflow:loss = 0.6027881, step = 4301 (2.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5635\n",
      "INFO:tensorflow:loss = 0.54152703, step = 4401 (2.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.0463\n",
      "INFO:tensorflow:loss = 0.66083395, step = 4501 (1.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.4703\n",
      "INFO:tensorflow:loss = 0.62198687, step = 4601 (2.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6755\n",
      "INFO:tensorflow:loss = 0.60302573, step = 4701 (1.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6348\n",
      "INFO:tensorflow:loss = 0.57599527, step = 4801 (1.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.0629\n",
      "INFO:tensorflow:loss = 0.59555376, step = 4901 (1.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.1622\n",
      "INFO:tensorflow:loss = 0.49892017, step = 5001 (2.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4043\n",
      "INFO:tensorflow:loss = 0.6800725, step = 5101 (2.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1188\n",
      "INFO:tensorflow:loss = 0.7758391, step = 5201 (2.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.4261\n",
      "INFO:tensorflow:loss = 0.5849468, step = 5301 (2.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6492\n",
      "INFO:tensorflow:loss = 0.67116183, step = 5401 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4597\n",
      "INFO:tensorflow:loss = 0.6721305, step = 5501 (2.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7385\n",
      "INFO:tensorflow:loss = 0.58171046, step = 5601 (2.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.6575\n",
      "INFO:tensorflow:loss = 0.5937742, step = 5701 (2.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6596\n",
      "INFO:tensorflow:loss = 0.6021945, step = 5801 (2.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.8168\n",
      "INFO:tensorflow:loss = 0.5518951, step = 5901 (2.717 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmp1tui31ju/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6435217.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1235e78d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-10-14:44:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/n8/wbjbrw4n6wv8v5kbx4zg70wm0000gn/T/tmp1tui31ju/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [20/200]\n",
      "INFO:tensorflow:Evaluation [40/200]\n",
      "INFO:tensorflow:Evaluation [60/200]\n",
      "INFO:tensorflow:Evaluation [80/200]\n",
      "INFO:tensorflow:Evaluation [100/200]\n",
      "INFO:tensorflow:Evaluation [120/200]\n",
      "INFO:tensorflow:Evaluation [140/200]\n",
      "INFO:tensorflow:Evaluation [160/200]\n",
      "INFO:tensorflow:Evaluation [180/200]\n",
      "INFO:tensorflow:Evaluation [200/200]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-10-14:44:58\n",
      "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.79, auc = 0.6002502, global_step = 6000, loss = 0.54535925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.79, 'auc': 0.6002502, 'loss': 0.54535925, 'global_step': 6000}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(input_fn=eval_input_fn, steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved test set accuracy of 79% which is better than most models for the Titanic dataset without any feature engineering. Concluding this guide we have seen an example of the new input pipelines that Google introduced recently and how they tie into the estimator models. For more extensive examples and tutorials I strongly advise to visit the latest version [docs](https://www.tensorflow.org/get_started/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
